{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network (manual)\n",
    "\n",
    "Manual implementation of a multilayer NN (Deep NN)\n",
    "\n",
    "No gradient descent loop. Just code for one iteration over all examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases import *\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "- 2-layer case: The model's structure is: [LINEAR -> RELU] -> [LINEAR -> SIGMOID]\n",
    "- L-layer case: [LINEAR -> RELU] Ã— (L-1) -> [LINEAR -> SIGMOID]\n",
    "\n",
    "**Weight/bias matrix sizes:**\n",
    "\n",
    "Each input goes to each neuron in first layer\n",
    "\n",
    "--> W1 size is L1_neuron_count x input_size\n",
    "\n",
    "--> W2 size is L2_neuron_coutn x L1_neuron_count..etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c468c89deb6d0cacf2ade5ab4151d26e",
     "grade": false,
     "grade_id": "cell-96d4e144d9419b32",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 2-layer case\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    # numpy randn gives random numbers \n",
    "    # from the normal (gaussian) distribution\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))   \n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "# L-layer case\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        \n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cce3e70ca32b353d4eddef1e3fbabcda",
     "grade": true,
     "grade_id": "cell-4b2bdbdd0f520c8d",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 2-layer case test\n",
      "W1 = [[ 0.01624345 -0.00611756 -0.00528172]\n",
      " [-0.01072969  0.00865408 -0.02301539]]\n",
      "b1 = [[0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.01744812 -0.00761207]]\n",
      "b2 = [[0.]]\n",
      "\n",
      "# L-layer case test\n",
      "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "#2-layer case test\n",
    "print('# 2-layer case test')\n",
    "parameters = initialize_parameters(3,2,1)\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "\n",
    "# L-layer case test\n",
    "print('\\n# L-layer case test')\n",
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "770763ab229ee87e8f5dfd520428caa3",
     "grade": false,
     "grade_id": "cell-4d6e09486a53f4c4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    sigmoid activation\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    assert(A.shape == Z.shape)   \n",
    "    \n",
    "    return A\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    return A\n",
    "\n",
    "# Linear\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.matmul(W, A) + b\n",
    "    \n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "# Activation\n",
    "def activation_forward(Z, activation):\n",
    "    \"\"\"\n",
    "    Implement the activation function g\n",
    "\n",
    "    Arguments:\n",
    "    Z -- the output of the linear forward function \n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing the \"activation_cache\"\n",
    "    \"\"\"\n",
    "    cache = Z\n",
    "    if activation == \"sigmoid\":\n",
    "        A = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "# Controller for forward propagation\n",
    "def forward_propagator(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep(). Can also be used to get the layer count.\n",
    "    \n",
    "    Returns:\n",
    "    AL -- activation value from the output (last) layer\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2  # number of layers in the neural network\n",
    "\n",
    "    # [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    # The for loop starts at 1 because layer 0 is the input\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        # Linear function\n",
    "        Z, linear_cache = linear_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)])\n",
    "        # Activation function\n",
    "        A, activation_cache = activation_forward(Z, \"relu\")\n",
    "        cache = (linear_cache, activation_cache)\n",
    "        caches.append(cache)\n",
    "        \n",
    "    # LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    # Linear function\n",
    "    Z, linear_cache = linear_forward(A, parameters['W' + str(L)], parameters['b' + str(L)])\n",
    "    # Activation function\n",
    "    AL, activation_cache = activation_forward(Z, \"sigmoid\")\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    caches.append(cache)\n",
    "          \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18d99d8170d2fed802a3e97e362339c6",
     "grade": true,
     "grade_id": "cell-ddc3a524cd1a0782",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n"
     ]
    }
   ],
   "source": [
    "# Forward prop test\n",
    "\n",
    "t_X, t_parameters = L_model_forward_test_case_2hidden()\n",
    "t_AL, t_caches = forward_propagator(t_X, t_parameters)\n",
    "\n",
    "print(\"AL = \" + str(t_AL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17919bb7d82635554b52aed7e96e8d9b",
     "grade": false,
     "grade_id": "cell-abad606772066f14",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Regression cost function (cuz last layer is a sigmoid one)\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    cost = -1/m * np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL))\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "913bc99f9f1380196c0f88d82d1af893",
     "grade": true,
     "grade_id": "cell-e82b9dd1fa6e970b",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.2797765635793422\n"
     ]
    }
   ],
   "source": [
    "t_Y, t_AL = compute_cost_test_case()\n",
    "t_cost = compute_cost(t_AL, t_Y)\n",
    "\n",
    "print(\"Cost: \" + str(t_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation\n",
    "\n",
    "For layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation).\n",
    "\n",
    "If we have the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. we can get $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$.\n",
    "\n",
    "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ are computed using the input $dZ^{[l]}$.\n",
    "\n",
    "Here are the formulas you need:\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n",
    "\n",
    "\n",
    "$A^{[l-1] T}$ is the transpose of $A^{[l-1]}$.\n",
    "\n",
    "### activation backwards prop\n",
    "If $g(.)$ is the activation function, \n",
    "`sigmoid_backward` and `relu_backward` compute $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}). \\tag{11}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivative of the Logistic Regression cost function \n",
    "def cost_function_derivative(Y, AL):\n",
    "    \"\"\"\n",
    "    dAL (dJ/dAL : last layer) is the derivative of the logistic regression\n",
    "    cost function with respect to AL (= y_hat)\n",
    "    \"\"\"\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "    return dAL\n",
    "\n",
    "# g'(Z) depending on activation type \n",
    "def activation_function_derivative(Z, activation_type):\n",
    "    \"\"\"\n",
    "    Returns derivative of the activation function of a particular layer l(?)\n",
    "    gl`(Zl)\n",
    "    \"\"\"\n",
    "    if activation_type == \"relu\":\n",
    "        # g(Z) = max(0, Z) --> derivative is 1 when Z >0 and is 0 otherwise\n",
    "        g_prime = np.where(Z > 0, 1, 0)\n",
    "        \n",
    "    elif activation_type == \"sigmoid\":\n",
    "        # derivative of sigmoid (s) is  s * (1-s)\n",
    "        s = sigmoid(Z)\n",
    "        g_prime = s * (1-s)  \n",
    "    return g_prime\n",
    "\n",
    "# dW, db, dA_prev\n",
    "def get_gradients(dA_l, cache, activation):\n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "    Z_l = activation_cache\n",
    "    m = Z_l.shape[1]\n",
    "    A_prev, W_l, b_l = linear_cache\n",
    "    g_prime_l = activation_function_derivative(Z_l, activation)    \n",
    "    dZ_l = dA_l * g_prime_l\n",
    "\n",
    "    dA_prev_l = np.matmul(W_l.T, dZ_l)\n",
    "    dW_l = 1/m * np.matmul(dZ_l, A_prev.T)\n",
    "    db_l = 1/m * np.sum(dZ_l, axis=1, keepdims=True)\n",
    "    \n",
    "    return dA_prev_l, dW_l, db_l\n",
    "\n",
    "# Controller for backwards propagation\n",
    "def backward_propagator(A_L, Y, caches):\n",
    "    \"\"\"\n",
    "    Backward propagation for a [LINEAR->RELU] * (L-1) -> [LINEAR -> SIGMOID] NN\n",
    "    \n",
    "    Arguments:\n",
    "    A_L -- probability vector, output of the forward propagation (also equals y_hat)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"    \n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = A_L.shape[1]\n",
    "    Y = Y.reshape(A_L.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    dA_L = cost_function_derivative(Y, A_L) # derivative of cost with respect to AL\n",
    "    cache_L = caches[-1] # cache for last layer\n",
    "    dA_prev_L, dW_L, db_L = get_gradients(dA_L, cache_L, \"sigmoid\")\n",
    "\n",
    "    grads[\"dA\" + str(L-1)] = dA_prev_L\n",
    "    grads[\"dW\" + str(L)] = dW_L\n",
    "    grads[\"db\" + str(L)] = db_L\n",
    "    \n",
    "    # From l=L-1 to 1\n",
    "    for l in reversed(range(1, L)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        \n",
    "        cache_l = caches[l-1] # list indices are 0~L-1, while layer naming is 1~L (for layer l, index is l-1)\n",
    "        dA_l = grads[\"dA\" + str(l)]\n",
    "        \n",
    "        dA_prev_l, dW_l, db_l = get_gradients(dA_l, cache_l, \"relu\")\n",
    "        \n",
    "        grads[\"dA\" + str(l - 1)] = dA_prev_l\n",
    "        grads[\"dW\" + str(l)] = dW_l\n",
    "        grads[\"db\" + str(l)] = db_l\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA0 = [[ 0.          0.52257901]\n",
      " [ 0.         -0.3269206 ]\n",
      " [ 0.         -0.32070404]\n",
      " [ 0.         -0.74079187]]\n",
      "dA1 = [[ 0.12913162 -0.44014127]\n",
      " [-0.14175655  0.48317296]\n",
      " [ 0.01663708 -0.05670698]]\n",
      "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
      "dW2 = [[-0.39202432 -0.13325855 -0.04601089]]\n",
      "db1 = [[-0.22007063]\n",
      " [ 0.        ]\n",
      " [-0.02835349]]\n",
      "db2 = [[0.15187861]]\n"
     ]
    }
   ],
   "source": [
    "## backprop Test\n",
    "t_AL, t_Y_assess, t_caches = L_model_backward_test_case()\n",
    "grads = backward_propagator(t_AL, t_Y_assess, t_caches)\n",
    "\n",
    "\n",
    "print(\"dA0 = \" + str(grads['dA0']))\n",
    "print(\"dA1 = \" + str(grads['dA1']))\n",
    "print(\"dW1 = \" + str(grads['dW1']))\n",
    "print(\"dW2 = \" + str(grads['dW2']))\n",
    "print(\"db1 = \" + str(grads['db1']))\n",
    "print(\"db2 = \" + str(grads['db2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a497191ef80b70967006d707cac91c20",
     "grade": false,
     "grade_id": "cell-3cb535f16aba3339",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    params -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    parameters = params.copy()\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0606cae114ec47754dc5383bc3dcdea",
     "grade": true,
     "grade_id": "cell-139de12ee845c39c",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "# Parameter update test\n",
    "t_parameters, grads = update_parameters_test_case()\n",
    "t_parameters = update_parameters(t_parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(t_parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(t_parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(t_parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(t_parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
